{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c236a8f8",
   "metadata": {
    "id": "c236a8f8"
   },
   "source": [
    "# Collect some tweets from Twitter\n",
    "We use snscrape library to retrieve tweets from twitter, and since the project is about classifying the tweets as offensive and non-offensive, we restricted our search by using a list of words and phrases (it will give us only the tweets that contains one or more of these words and phrases). Then, we used pandas to save these tweets as a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50315a32",
   "metadata": {
    "id": "50315a32"
   },
   "source": [
    "```python\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import tqdm             # to show the progress of the operation\n",
    "\n",
    "queries = ['flower', 'plant', 'cloud', 'montain', 'sea', 'moon', 'sand',\n",
    "           'motherfucker', 'dickhead', 'dick','fat', 'suck my dick',\n",
    "           'fucker', 'kill your self', 'niggers', 'bang your mom',\n",
    "           'whore', 'fuck you','hope you die', 'just die', 'death',\n",
    "           'kill', 'fuck', 'shit', 'pussy', 'cancer', 'love','hate',\n",
    "           'moron', 'balls', 'suck my balls', 'rape', 'asshole','bitch']\n",
    "tweets = []\n",
    "limit = 1500   # for each phrase or word\n",
    "\n",
    "for query in tqdm.tqdm(queries):\n",
    "    count = 0\n",
    "    #search tweets for each word or phrase from the list at a time\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():     \n",
    "        if count  == limit:\n",
    "            break\n",
    "        else:\n",
    "            # using just the content (text)of the tweet\n",
    "            tweets.append([tweet.content])\n",
    "            count += 1\n",
    "\n",
    "df = pd.DataFrame(tweets, columns = ['Tweet'])\n",
    "\n",
    "df.to_csv('n_data.csv', index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ecfa8",
   "metadata": {
    "id": "484ecfa8"
   },
   "source": [
    "# Data preprocessing\n",
    "In this stage and after labeling the tweets manually, we start to clean the data to be used it in our models. First, we remove the usernames and website addresses and any links (contents that do not provide any information to our model) from the tweets, and add these tweets to the empty Dataframe that we created using pandas. Next, we eliminate the duplications and null values from our Dataframe. The model cannot work on tweets with different languages so we use LangeDetect library to recognize and eliminate the non-English words and phrases from tweets. Now our dataset is preprocessed and ready to be used in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c5cb8",
   "metadata": {
    "id": "d80c5cb8"
   },
   "source": [
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import tqdm\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# create an empty Dataframe\n",
    "prep_df = pd.DataFrame(columns =['Tweet', 'Label'])\n",
    "\n",
    "print(\"Deleting users name and urls:\")\n",
    "\n",
    "for tweet in tqdm.tqdm(df['Tweet']):\n",
    "    tweet_words = []\n",
    "    for word in str(tweet).split(' '):\n",
    "        if word.startswith('@'):   # removing usernames\n",
    "            word = ''\n",
    "        elif word.startswith('http'):   # removing websites links\n",
    "            word = ''\n",
    "        elif word.startswith('https'):\n",
    "            word = ''\n",
    "        tweet_words.append(word)\n",
    "    prep_tweet = \" \".join(tweet_words)    \n",
    "    # adding the cleaned tweets to the empty Dataframe\n",
    "    prep_df = pd.concat( [prep_df, pd.DataFrame({\"Tweet\": [prep_tweet],\n",
    "                                                 \"Label\": 0})],\n",
    "                        ignore_index = True)\n",
    "\n",
    "prep_df = prep_df.drop_duplicates()      # deleting the repeated tweets\n",
    "\n",
    "prep_df = prep_df.dropna()               # delete the null values\n",
    "\n",
    "print(\"Deleting non english tweets:\")\n",
    "\n",
    "tweets = prep_df['Tweet']\n",
    "\n",
    "for tweet in tqdm.tqdm(prep_df['Tweet']):\n",
    "    try:\n",
    "        language = detect(tweet)  # to detect the language of the text\n",
    "    except:\n",
    "        prep_df = prep_df[prep_df['Tweet']!=tweet]\n",
    "        continue\n",
    "    if language != 'en':\n",
    "        # removing the non-English tweets\n",
    "        prep_df = prep_df[prep_df['Tweet']!=tweet]\n",
    "\n",
    "print(\"Number of total preprocessed data:\", len(prep_df))\n",
    "\n",
    "prep_df = prep_df.drop_duplicates(subset=['Tweet'])\n",
    "\n",
    "prep_df.to_csv('prep_data.csv', index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b43321",
   "metadata": {
    "id": "51b43321"
   },
   "source": [
    "# Working on the classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d90f1",
   "metadata": {
    "id": "d94d90f1"
   },
   "source": [
    "## Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84db9c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f84db9c0",
    "outputId": "e1eaa43d-b014-4696-9de8-28bd174ac9bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jrhin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jrhin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, PredefinedSplit\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import word_tokenize; nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f469b7",
   "metadata": {
    "id": "b4f469b7"
   },
   "source": [
    "## Loading the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa383c8",
   "metadata": {
    "id": "6aa383c8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled_data.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062e3e7",
   "metadata": {
    "id": "f062e3e7"
   },
   "source": [
    "## Visualizating the data\n",
    "shows how the data distributed, as it can be seen we have almost an balanced data. The chart shows the percentage of the two categories, 51.5% (a bit more than the half) of the data are labeles as non-offensive and 48.5% are as offensive, which means they are almost evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfecf44",
   "metadata": {
    "id": "2dfecf44",
    "outputId": "59ccddf7-7bd2-4433-d675-2364a48db92d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in the spirit of gender equality, i will now b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>motherfucker really said “do me a favor”</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>About is about perspective they say, but in a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40 fucks in mine! 11 goddamns. 21 shits. And...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Motherfucker you are exactly the reason why p...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Label\n",
       "0  in the spirit of gender equality, i will now b...    0.0\n",
       "1          motherfucker really said “do me a favor”     1.0\n",
       "2   About is about perspective they say, but in a...    1.0\n",
       "3    40 fucks in mine! 11 goddamns. 21 shits. And...    1.0\n",
       "4   Motherfucker you are exactly the reason why p...    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 17736\n",
      "Distribution of the labels:\n",
      "0.0    9136\n",
      "1.0    8600\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())\n",
    "\n",
    "print(\"Number of tweets:\", len(data))\n",
    "print(\"Distribution of the labels:\")\n",
    "print(data.Label.value_counts())\n",
    "plt.pie(list(data.Label.value_counts()/len(data.Label)), startangle=90, labels = ['Non Offensive', 'Offensive'], autopct='%1.1f%%')\n",
    "plt.title('Distribution of the labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42670a7",
   "metadata": {
    "id": "e42670a7"
   },
   "source": [
    "## Defining $x$ and $y$, and then deviding them in training, validation and testing set\n",
    "\n",
    "The column `Tweet` is the content (text) of tweets so it is the festures of data), and the column `Label` demonstrates the categories of those tweets. \n",
    "We, first split the whole data into two dataset : `train set` (15075 rows) and `test set` (0.15 of the data (2661 rows) that will be used to evaluate the performance of the models (classifiers) on unseen data by predicting their labels), then we split the Train set into two subsets : `train set` (used to train the model) and  `val set`( 0.18 of the train set from the first split = 2714).\n",
    "\n",
    "At the end, we have set a grid search using `PredifinedSplit` method from `sklean` by defining the index `0` for `validation set` and `-1` for `train set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54af79e4",
   "metadata": {
    "id": "54af79e4"
   },
   "outputs": [],
   "source": [
    "x = data.Tweet.to_numpy()\n",
    "y = data.Label.to_numpy()\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(x, y, test_size = 0.15, shuffle = True, random_state = 42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.18, shuffle=True, random_state = 42)\n",
    "\n",
    "split_index = [-1 if elem in x_train else 0 for elem in X_train]  # implementing a grid search using PredefinedSplit method \n",
    "ps = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35001169",
   "metadata": {
    "id": "35001169"
   },
   "source": [
    "# Using *TfidfVectorizer* \n",
    "\n",
    "Since the Machine Learning algorithms deal with numbers and take vectors of numeric features, we need to transform the text data into numeric vactors to be able to use it in these algorithms; for this purpose, we defined a vectorizer using the `TfidfVectorizer` method from `sklearn` library to extract the features from the text (`tf-idf` means term-frequency times inverse document-frequency, and the goal of using this method is to scale down the impact of tokens that occur very frequently in a given corpus(set) and that empirically provides less information than features that occur in a small fraction of the training corpus).  \n",
    "\n",
    "The `EnglishStemmer` method form `nltk` libarry (a Natural Language Processing libarary to process the text data) helped us stem the words to their root so instead of processing different type of a word (containing prefix, suffix, ...), it deals with just the root.\n",
    "\n",
    "A function also is defined to remove the `stop words`, because by removing these frequently used words, the focus will be given to the words the define the meaning of the text.Therefore, after defining the stopwords using `nltk` library, we remove them from the text using `EnglishStemmer` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343b3436",
   "metadata": {
    "id": "343b3436"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', preprocessor = None)   #define vectorizer\n",
    "\n",
    "stemmer = EnglishStemmer()                                                   #define stemmer\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))                          # define the stopwords using the stopwords methods in Nltk library \n",
    "\n",
    "#functions for stemming\n",
    "def stemming_tokenizer(text):\n",
    "\t# stemming each word to its root by removing the prefix and suffix  \n",
    "\tstemmed_text = [stemmer.stem(word) for word in word_tokenize(text, language='english')]\n",
    "\treturn stemmed_text\n",
    "\n",
    "def stemming_stop_tokenizer(text):\n",
    "\t# removing the stop words\n",
    "\tstemmed_text = [stemmer.stem(word) for word in word_tokenize(text, language='english') if word not in english_stopwords]\n",
    "\treturn stemmed_text\n",
    "#dictionary containing configuration\n",
    "conf = {'vect__tokenizer': [None, stemming_tokenizer, stemming_stop_tokenizer], 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], 'vect__min_df': [0.0, 0.1, 0.2]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c91666",
   "metadata": {
    "id": "27c91666"
   },
   "source": [
    "## Defining the classifiers and their paramenters\n",
    "\n",
    "After importing the classifier from `sklearn` library,here we define each of them by setting the its main parameter. We have used numbers of classifier to compare their performance in analysing text data using different hypeparameters.\n",
    "\n",
    "List of used Classifiers:\n",
    "\n",
    "- `The multinomial Naive Bayes classifier` is normally used with disceret features, but it also works well on fractional feature (tf-idf features)\n",
    "\n",
    "- `KNeighborsClassifier` \n",
    "- `Support Vector Machine`\n",
    "- `DecisionTreeClassifier`\n",
    "- `RandomForestClassifier`\n",
    "- `LogisticRegression`\n",
    "- `GradientBoostingClassifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826592b9",
   "metadata": {
    "id": "826592b9"
   },
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "KNN = KNeighborsClassifier(n_jobs = -1)\n",
    "SVM = SVC(random_state = 42)\n",
    "DT = DecisionTreeClassifier(random_state = 42)\n",
    "RF = RandomForestClassifier(random_state = 42)\n",
    "LR = LogisticRegression(random_state = 42)\n",
    "GBC = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "MNB_param = {'MNB__alpha': [0.01, 0.1, 1.0]}    \n",
    "KNN_param = {'KNN__n_neighbors': [3, 5, 10]}\n",
    "SVM_param = {'SVM__kernel': ['poly', 'sigmoid']}\n",
    "DT_param = {'DT__criterion': ['gini', 'entropy']}\n",
    "RF_param = {'RF__criterion': ['gini', 'entropy']}\n",
    "LR_param = {'LR__penalty': ['l1', 'l2', 'elasticnet', 'none']}\n",
    "GBC_param = {'GBC__loss': ['log_loss', 'exponential'], 'GBC__learning_rate': [0.1, 0.01, 0.001]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b844920",
   "metadata": {
    "id": "7b844920"
   },
   "source": [
    "## Performing a randomized search for each classifier\n",
    "\n",
    "After defining the classsifiers and setting a range for their parameter, we have made a pipeline for each model applying a grid search to find the best parameter. In the grid search and for each model, we compare the `f1` score derived from different values for the parameter using the data split that is done using `PredifinedSplit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe6f9f7",
   "metadata": {
    "id": "1fe6f9f7",
    "outputId": "5b0e4424-a4e7-49fa-a96d-c9531fb3a45b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 382, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 382, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.74500768        nan        nan\n",
      " 0.87390476 0.50189474 0.56575496        nan 0.75474507        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0, -1])),\n",
       "                   estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                              TfidfVectorizer(strip_accents=&#x27;ascii&#x27;)),\n",
       "                                             (&#x27;GBC&#x27;,\n",
       "                                              GradientBoostingClassifier(random_state=42))]),\n",
       "                   n_iter=15,\n",
       "                   param_distributions={&#x27;GBC__learning_rate&#x27;: [0.1, 0.01,\n",
       "                                                               0.001],\n",
       "                                        &#x27;GBC__loss&#x27;: [&#x27;log_loss&#x27;,\n",
       "                                                      &#x27;exponential&#x27;],\n",
       "                                        &#x27;vect__min_df&#x27;: [0.0, 0.1, 0.2],\n",
       "                                        &#x27;vect__ngram_range&#x27;: [(1, 1), (1, 2),\n",
       "                                                              (1, 3)],\n",
       "                                        &#x27;vect__tokenizer&#x27;: [None,\n",
       "                                                            &lt;function stemming_tokenizer at 0x7fc085981d80&gt;,\n",
       "                                                            &lt;function stemming_stop_tokenizer at 0x7fc085981ea0&gt;]},\n",
       "                   random_state=42, scoring=&#x27;f1&#x27;, verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0, -1])),\n",
       "                   estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                              TfidfVectorizer(strip_accents=&#x27;ascii&#x27;)),\n",
       "                                             (&#x27;GBC&#x27;,\n",
       "                                              GradientBoostingClassifier(random_state=42))]),\n",
       "                   n_iter=15,\n",
       "                   param_distributions={&#x27;GBC__learning_rate&#x27;: [0.1, 0.01,\n",
       "                                                               0.001],\n",
       "                                        &#x27;GBC__loss&#x27;: [&#x27;log_loss&#x27;,\n",
       "                                                      &#x27;exponential&#x27;],\n",
       "                                        &#x27;vect__min_df&#x27;: [0.0, 0.1, 0.2],\n",
       "                                        &#x27;vect__ngram_range&#x27;: [(1, 1), (1, 2),\n",
       "                                                              (1, 3)],\n",
       "                                        &#x27;vect__tokenizer&#x27;: [None,\n",
       "                                                            &lt;function stemming_tokenizer at 0x7fc085981d80&gt;,\n",
       "                                                            &lt;function stemming_stop_tokenizer at 0x7fc085981ea0&gt;]},\n",
       "                   random_state=42, scoring=&#x27;f1&#x27;, verbose=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(strip_accents=&#x27;ascii&#x27;)),\n",
       "                (&#x27;GBC&#x27;, GradientBoostingClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(strip_accents=&#x27;ascii&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0, -1])),\n",
       "                   estimator=Pipeline(steps=[('vect',\n",
       "                                              TfidfVectorizer(strip_accents='ascii')),\n",
       "                                             ('GBC',\n",
       "                                              GradientBoostingClassifier(random_state=42))]),\n",
       "                   n_iter=15,\n",
       "                   param_distributions={'GBC__learning_rate': [0.1, 0.01,\n",
       "                                                               0.001],\n",
       "                                        'GBC__loss': ['log_loss',\n",
       "                                                      'exponential'],\n",
       "                                        'vect__min_df': [0.0, 0.1, 0.2],\n",
       "                                        'vect__ngram_range': [(1, 1), (1, 2),\n",
       "                                                              (1, 3)],\n",
       "                                        'vect__tokenizer': [None,\n",
       "                                                            <function stemming_tokenizer at 0x7fc085981d80>,\n",
       "                                                            <function stemming_stop_tokenizer at 0x7fc085981ea0>]},\n",
       "                   random_state=42, scoring='f1', verbose=-1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = {} \n",
    "\n",
    "# make pipeline for MNB\n",
    "pipeline= Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('MNB', MNB),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **MNB_param} #union of the dicts\n",
    "\n",
    "grid_search['MNB'] = RandomizedSearchCV(pipeline, parameters, scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['MNB'].fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# make pipeline for KNN\n",
    "pipeline = Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('KNN', KNN),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **KNN_param}\n",
    "\n",
    "grid_search['KNN'] = RandomizedSearchCV(pipeline, parameters,  scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['KNN'].fit(X_train, Y_train)\n",
    "\n",
    "# make pipeline for SVM\n",
    "pipeline = Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('SVM', SVM),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **SVM_param}\n",
    "\n",
    "grid_search['SVM'] = RandomizedSearchCV(pipeline, parameters,  scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['SVM'].fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# make pipeline for DT\n",
    "pipeline = Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('DT', DT),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **DT_param}\n",
    "\n",
    "grid_search['DT'] = RandomizedSearchCV(pipeline, parameters,  scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['DT'].fit(X_train, Y_train)\n",
    "\n",
    "# make pipeline for RF\n",
    "pipeline = Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('RF', RF),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **RF_param}\n",
    "\n",
    "grid_search['RF'] = RandomizedSearchCV(pipeline, parameters,  scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['RF'].fit(X_train, Y_train)\n",
    "\n",
    "# make pipeline for LR\n",
    "pipeline= Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('LR', LR),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **LR_param} #union of the dicts\n",
    "\n",
    "grid_search['LR'] = RandomizedSearchCV(pipeline, parameters, scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['LR'].fit(X_train, Y_train)\n",
    "\n",
    "# make pipeline for GBC\n",
    "pipeline= Pipeline([\n",
    "\t('vect', vectorizer),\n",
    "\t('GBC', GBC),\n",
    "\t])\n",
    "\n",
    "parameters = {**conf, **GBC_param} #union of the dicts\n",
    "\n",
    "grid_search['GBC'] = RandomizedSearchCV(pipeline, parameters, scoring = 'f1',\n",
    "                                     cv = ps, n_iter = 15, verbose = -1, random_state = 42)\n",
    "\n",
    "grid_search['GBC'].fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87c183",
   "metadata": {
    "id": "7d87c183"
   },
   "source": [
    "## Plotting the results\n",
    "Here, we show the best results of each classifier that is obtained from the grid serach using the best parameters, and by looking at the result we can see that the SVM classifier performed better than the other classifier by acheiving the highest f1 score (90%). It means the SVM model classified 90% of test data correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1099c5c1",
   "metadata": {
    "id": "1099c5c1",
    "outputId": "acb1b2d4-0958-4d47-c7aa-5b0ccf0ed297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores MNB:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.84      0.80      0.82      1361\n",
      "    Offensive       0.80      0.84      0.82      1300\n",
      "\n",
      "     accuracy                           0.82      2661\n",
      "    macro avg       0.82      0.82      0.82      2661\n",
      " weighted avg       0.82      0.82      0.82      2661\n",
      "\n",
      "\n",
      "Confusion Matrix MNB:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1092</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1092   269\n",
       "1   212  1088"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores KNN:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.80      0.74      0.77      1361\n",
      "    Offensive       0.75      0.80      0.77      1300\n",
      "\n",
      "     accuracy                           0.77      2661\n",
      "    macro avg       0.77      0.77      0.77      2661\n",
      " weighted avg       0.77      0.77      0.77      2661\n",
      "\n",
      "\n",
      "Confusion Matrix KNN:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1004   357\n",
       "1   256  1044"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.91      0.89      0.90      1361\n",
      "    Offensive       0.89      0.91      0.90      1300\n",
      "\n",
      "     accuracy                           0.90      2661\n",
      "    macro avg       0.90      0.90      0.90      2661\n",
      " weighted avg       0.90      0.90      0.90      2661\n",
      "\n",
      "\n",
      "Confusion Matrix SVM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1210</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1210   151\n",
       "1   115  1185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores DT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.89      0.87      0.88      1361\n",
      "    Offensive       0.87      0.89      0.88      1300\n",
      "\n",
      "     accuracy                           0.88      2661\n",
      "    macro avg       0.88      0.88      0.88      2661\n",
      " weighted avg       0.88      0.88      0.88      2661\n",
      "\n",
      "\n",
      "Confusion Matrix DT:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1186</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1186   175\n",
       "1   147  1153"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores RF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.88      0.90      0.89      1361\n",
      "    Offensive       0.89      0.87      0.88      1300\n",
      "\n",
      "     accuracy                           0.88      2661\n",
      "    macro avg       0.88      0.88      0.88      2661\n",
      " weighted avg       0.88      0.88      0.88      2661\n",
      "\n",
      "\n",
      "Confusion Matrix RF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1221   140\n",
       "1   170  1130"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores LR:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.88      0.89      0.89      1361\n",
      "    Offensive       0.89      0.87      0.88      1300\n",
      "\n",
      "     accuracy                           0.88      2661\n",
      "    macro avg       0.88      0.88      0.88      2661\n",
      " weighted avg       0.88      0.88      0.88      2661\n",
      "\n",
      "\n",
      "Confusion Matrix LR:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1214</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>1136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1214   147\n",
       "1   164  1136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores GBC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.88      0.89      0.88      1361\n",
      "    Offensive       0.88      0.88      0.88      1300\n",
      "\n",
      "     accuracy                           0.88      2661\n",
      "    macro avg       0.88      0.88      0.88      2661\n",
      " weighted avg       0.88      0.88      0.88      2661\n",
      "\n",
      "\n",
      "Confusion Matrix GBC:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1205</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1205   156\n",
       "1   160  1140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in grid_search:\n",
    "    pred = grid_search[classifier].predict(x_test)\n",
    "    print(\"Scores\", classifier+\":\")\n",
    "    print(metrics.classification_report(y_test, pred, target_names = [\"Non offensive\", \"Offensive\"]))\n",
    "    print()\n",
    "    print(\"Confusion Matrix\", classifier+\":\")\n",
    "    display(pd.DataFrame(metrics.confusion_matrix(y_test, pred)))\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750fba5",
   "metadata": {
    "id": "a750fba5"
   },
   "source": [
    "# Transformers encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627185a3",
   "metadata": {
    "id": "627185a3"
   },
   "source": [
    "## Downloading *all-distilroberta-v1* and perform a grid search for each classifier\n",
    "\n",
    "Here, we have used a specific kind of encoding method called `transformers encoding` (or `positional encoding`). In this method, the information regarding the position of an object in a series is maintained, and since the position of a word can affect the meaning of the sentence, by using this encoding method we can retain and use this valuable information. The positional encoding assign each position or index to a vector so the output of this encoding is a matrix. This method is applied using `sentences_transformer` library and the resulted data is a matrix with 768 dimensions, which is hard to be processed. Therefore, a PCA algorith is used to reduced these dimensions, then the compressed data is used to train our clasifiers by applying a grid search to find the best combination of parameters with the highest `f1 score`.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9d38a5",
   "metadata": {
    "id": "2f9d38a5",
    "outputId": "6c27b514-1001-4ba6-d8a5-52c29b2f73a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components of the pca: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "2 fits failed out of a total of 4.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/jrhin/Uni/sl/slenv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.82623705        nan 0.82122261]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MNB_param = {'alpha': [0.01, 0.1, 1.0]}    \n",
    "KNN_param = {'n_neighbors': [3, 5, 10]}\n",
    "SVM_param = {'kernel': ['poly', 'sigmoid']}\n",
    "DT_param = {'criterion': ['gini', 'entropy']}\n",
    "RF_param = {'criterion': ['gini', 'entropy']}\n",
    "LR_param = {'penalty': ['l1', 'l2', 'elasticnet', 'none']}\n",
    "GBC_param = {'loss': ['log_loss', 'exponential'], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# We donwload a new pretrained model\n",
    "model = SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "# We encode the sets\n",
    "embeddings_train = model.encode(list(X_train))\n",
    "embeddings_test = model.encode(list(x_test))\n",
    "\n",
    "# We run the PCA\n",
    "pca = PCA(0.95, random_state = 42)\n",
    "pca.fit(embeddings_train)\n",
    "print(\"Number of components of the pca:\", pca.n_components_)\n",
    "# We transfrom the data using the PCA\n",
    "embeddings_train = pca.transform(embeddings_train)\n",
    "embeddings_test = pca.transform(embeddings_test)\n",
    "\n",
    "# A dictionary with the classifiers\n",
    "classifiers = {'SVM': GridSearchCV(SVC(random_state = 42), SVM_param,  scoring = 'f1', cv = ps, verbose = -1),\n",
    "               'KNN': GridSearchCV(KNeighborsClassifier(), KNN_param,  scoring = 'f1', cv = ps, verbose = -1),\n",
    "               'DecisionTree': GridSearchCV(DecisionTreeClassifier(random_state = 42), DT_param,  scoring = 'f1', cv = ps, verbose = -1),\n",
    "               'RandomForest': GridSearchCV(RandomForestClassifier(random_state = 42), RF_param,  scoring = 'f1', cv = ps, verbose = -1),\n",
    "               'LogisticRegression': GridSearchCV(LogisticRegression(random_state = 42), LR_param,  scoring = 'f1', cv = ps, verbose = -1),\n",
    "               'GradientBoostingClassifier': GridSearchCV(GradientBoostingClassifier(random_state = 42), GBC_param,  scoring = 'f1', cv = ps, verbose = -1)\n",
    "               }\n",
    "\n",
    "best_clf = None\n",
    "best_f1 = 0\n",
    "current_f1 = 0\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifiers[classifier].fit(embeddings_train, Y_train)\n",
    "    current_f1 = classifiers[classifier].best_score_\n",
    "\n",
    "    if best_f1 < current_f1:\n",
    "        best_f1 = current_f1\n",
    "        best_clf = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526ba28",
   "metadata": {
    "id": "0526ba28"
   },
   "source": [
    "## Print the results\n",
    "The results of classifiers trained by encoded data is shown bellow, and similar to the previous training, the Support Vector Machine classifier had a better performance with 90% `f1 score` comparing to the other classifiers. But, as we can see the model resulted in a better performance without `transformer encoding`, which is considered to be suprising due the fact that `positional encoding` is a powerful tool to retain valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a21e67",
   "metadata": {
    "id": "74a21e67",
    "outputId": "c6427299-4783-48b7-9e0d-8d8cbcf5697d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.88      0.85      0.86      1361\n",
      "    Offensive       0.85      0.88      0.86      1300\n",
      "\n",
      "     accuracy                           0.86      2661\n",
      "    macro avg       0.86      0.86      0.86      2661\n",
      " weighted avg       0.86      0.86      0.86      2661\n",
      "\n",
      "\n",
      "Confusion Matrix SVM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1151</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1151   210\n",
       "1   155  1145"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores KNN:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.82      0.82      0.82      1361\n",
      "    Offensive       0.81      0.81      0.81      1300\n",
      "\n",
      "     accuracy                           0.81      2661\n",
      "    macro avg       0.81      0.81      0.81      2661\n",
      " weighted avg       0.81      0.81      0.81      2661\n",
      "\n",
      "\n",
      "Confusion Matrix KNN:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1112   249\n",
       "1   245  1055"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores DecisionTree:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.72      0.71      0.72      1361\n",
      "    Offensive       0.70      0.71      0.70      1300\n",
      "\n",
      "     accuracy                           0.71      2661\n",
      "    macro avg       0.71      0.71      0.71      2661\n",
      " weighted avg       0.71      0.71      0.71      2661\n",
      "\n",
      "\n",
      "Confusion Matrix DecisionTree:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>973</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  973  388\n",
       "1  383  917"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores RandomForest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.79      0.83      0.81      1361\n",
      "    Offensive       0.81      0.77      0.79      1300\n",
      "\n",
      "     accuracy                           0.80      2661\n",
      "    macro avg       0.80      0.80      0.80      2661\n",
      " weighted avg       0.80      0.80      0.80      2661\n",
      "\n",
      "\n",
      "Confusion Matrix RandomForest:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1123   238\n",
       "1   298  1002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores LogisticRegression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.85      0.85      0.85      1361\n",
      "    Offensive       0.84      0.84      0.84      1300\n",
      "\n",
      "     accuracy                           0.85      2661\n",
      "    macro avg       0.85      0.85      0.85      2661\n",
      " weighted avg       0.85      0.85      0.85      2661\n",
      "\n",
      "\n",
      "Confusion Matrix LogisticRegression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1158</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1158   203\n",
       "1   207  1093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Scores GradientBoostingClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non offensive       0.84      0.82      0.83      1361\n",
      "    Offensive       0.81      0.83      0.82      1300\n",
      "\n",
      "     accuracy                           0.82      2661\n",
      "    macro avg       0.82      0.83      0.82      2661\n",
      " weighted avg       0.83      0.82      0.82      2661\n",
      "\n",
      "\n",
      "Confusion Matrix GradientBoostingClassifier:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1112   249\n",
       "1   217  1083"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    pred = classifiers[classifier].predict(embeddings_test)\n",
    "    print(\"Scores\", classifier+\":\")\n",
    "    print(metrics.classification_report(y_test, pred, target_names = [\"Non offensive\", \"Offensive\"]))\n",
    "    print()\n",
    "    print(\"Confusion Matrix\", classifier+\":\")\n",
    "    display(pd.DataFrame(metrics.confusion_matrix(y_test, pred)))\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Mario Edoardo Pandolfo"
   },
   {
    "name": "Hejaz Nawasser"
   },
   {
    "name": "Salim Sikder"
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "title": "Offensive tweets classification with classifiers"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
